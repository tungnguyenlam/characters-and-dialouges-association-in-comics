{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cae08cb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T05:37:07.500979Z",
     "iopub.status.busy": "2025-10-06T05:37:07.500302Z",
     "iopub.status.idle": "2025-10-06T05:37:07.947787Z",
     "shell.execute_reply": "2025-10-06T05:37:07.946979Z",
     "shell.execute_reply.started": "2025-10-06T05:37:07.500956Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n"
     ]
    }
   ],
   "source": [
    "!kill -9 $(nvidia-smi | grep python | awk '{print $5}') || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "627d836d-876d-4dbb-920b-74eed351e332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T05:37:07.949842Z",
     "iopub.status.busy": "2025-10-06T05:37:07.949584Z",
     "iopub.status.idle": "2025-10-06T05:37:08.393825Z",
     "shell.execute_reply": "2025-10-06T05:37:08.392971Z",
     "shell.execute_reply.started": "2025-10-06T05:37:07.949817Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct  6 05:37:08 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   66C    P0             29W /   70W |   15093MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   40C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4faa4db3-4786-4266-903d-1db11e6e6f40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T05:37:08.395186Z",
     "iopub.status.busy": "2025-10-06T05:37:08.394867Z",
     "iopub.status.idle": "2025-10-06T05:37:11.858177Z",
     "shell.execute_reply": "2025-10-06T05:37:11.857315Z",
     "shell.execute_reply.started": "2025-10-06T05:37:08.395138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --quiet ultralytics opencv-python matplotlib torch torchvision tqdm ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "076a2473",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T05:37:11.860677Z",
     "iopub.status.busy": "2025-10-06T05:37:11.860356Z",
     "iopub.status.idle": "2025-10-06T05:37:11.871064Z",
     "shell.execute_reply": "2025-10-06T05:37:11.870388Z",
     "shell.execute_reply.started": "2025-10-06T05:37:11.860642Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== System Information ===\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "\n",
      "Using single GPU: Tesla T4 (14.7 GB)\n",
      "CUDA version: 12.4\n",
      "\n",
      "CPU cores available: 4\n"
     ]
    }
   ],
   "source": [
    "# --- Import Required Libraries ---\n",
    "\n",
    "# File and data handling\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "# Data processing and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from ultralytics import YOLO\n",
    "from tqdm.notebook import tqdm  # For progress bars\n",
    "\n",
    "# Fix multiprocessing issues\n",
    "mp.set_start_method('spawn', force=True)  # Set start method to spawn\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')  # Use file_system sharing strategy\n",
    "\n",
    "# Set environment variable for memory allocation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "\n",
    "# Print system info\n",
    "print(\"=== System Information ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Force using only the first GPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Use both GPUs\n",
    "    # Verify GPU setup\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f\"\\nNumber of GPUs available: {n_gpus}\")\n",
    "    for i in range(n_gpus):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"\\nGPU {i}: {props.name}\")\n",
    "        print(f\"- Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
    "        print(f\"- CUDA Capability: {props.major}.{props.minor}\")\n",
    "    \n",
    "    # Set memory allocation strategy\n",
    "    torch.cuda.empty_cache()\n",
    "    for i in range(n_gpus):\n",
    "        torch.cuda.set_per_process_memory_fraction(0.7, i)  # Use 70% of available memory per GPU\n",
    "else:\n",
    "    print(\"WARNING: No GPU found. Training on CPU will be very slow!\")\n",
    "\n",
    "# Get CPU cores for parallel processing\n",
    "NUM_CORES = min(os.cpu_count(), 4)  # Limit number of CPU cores\n",
    "print(f\"\\nCPU cores to be used: {NUM_CORES}\")\n",
    "\n",
    "# Configure PyTorch for memory efficiency\n",
    "torch.backends.cudnn.benchmark = True  # Enable cuDNN auto-tuner\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # Allow TF32 on cudnn\n",
    "\n",
    "# Worker initialization function for DataLoader\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"Initialize worker process properly\"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7fd15b05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T05:37:11.871982Z",
     "iopub.status.busy": "2025-10-06T05:37:11.871795Z",
     "iopub.status.idle": "2025-10-06T05:37:11.889870Z",
     "shell.execute_reply": "2025-10-06T05:37:11.889117Z",
     "shell.execute_reply.started": "2025-10-06T05:37:11.871968Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset directories...\n",
      "Created: balloon_dataset/images/train\n",
      "Created: balloon_dataset/labels/train\n",
      "Created: balloon_dataset/images/val\n",
      "Created: balloon_dataset/labels/val\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Configuration Section ---\n",
    "\n",
    "# Paths to your data and model files\n",
    "JSON_DIR = '/kaggle/input/manga109-jsons/jsons'  # Thư mục chứa các file JSON\n",
    "IMAGE_ROOT_DIR = '/kaggle/input/manga109-images/images'  # Thư mục gốc chứa ảnh\n",
    "DATASET_DIR = 'balloon_dataset'  # Directory to store YOLO format dataset\n",
    "PRE_TRAINED_MODEL_DIR = 'pre-trained_model'\n",
    "\n",
    "# Validate paths\n",
    "print(\"\\nValidating directories...\")\n",
    "for path in [JSON_DIR, IMAGE_ROOT_DIR]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Directory not found: {path}\")\n",
    "    else:\n",
    "        print(f\"Found directory: {path}\")\n",
    "        # List some contents\n",
    "        contents = os.listdir(path)[:5]\n",
    "        print(f\"Sample contents: {contents}\")\n",
    "\n",
    "# Create dataset directories\n",
    "print(\"\\nCreating dataset directories...\")\n",
    "for split in ['train', 'val']:\n",
    "    for subdir in ['images', 'labels', 'masks']:  # Added masks directory\n",
    "        dir_path = os.path.join(DATASET_DIR, f'{subdir}/{split}')\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"Created: {dir_path}\")\n",
    "\n",
    "# Set category information directly\n",
    "TARGET_CATEGORY_ID = 5  # Fixed category ID for balloon\n",
    "TARGET_CATEGORY_NAME = \"balloon\"  # Fixed category name\n",
    "\n",
    "print(f\"\\nTarget Category Configuration:\")\n",
    "print(f\"Category ID: {TARGET_CATEGORY_ID}\")\n",
    "print(f\"Category Name: {TARGET_CATEGORY_NAME}\")\n",
    "\n",
    "# Optional: Verify category in a sample file\n",
    "print(\"\\nChecking JSON structure and categories...\")\n",
    "sample_files = [f for f in os.listdir(JSON_DIR) if f.endswith('.json')][:1]\n",
    "if sample_files:\n",
    "    sample_path = os.path.join(JSON_DIR, sample_files[0])\n",
    "    with open(sample_path, 'r') as f:\n",
    "        sample_data = json.load(f)\n",
    "        print(\"\\nJSON structure:\")\n",
    "        print(\"Keys:\", list(sample_data.keys()))\n",
    "        if 'images' in sample_data:\n",
    "            print(\"Sample image entry:\", sample_data['images'][0])\n",
    "        if 'annotations' in sample_data:\n",
    "            print(\"Sample annotation entry:\", sample_data['annotations'][0])\n",
    "        \n",
    "        # Check if our target category is present in annotations\n",
    "        if 'annotations' in sample_data:\n",
    "            balloon_annotations = [ann for ann in sample_data['annotations'] \n",
    "                                if ann['category_id'] == TARGET_CATEGORY_ID]\n",
    "            if balloon_annotations:\n",
    "                print(f\"\\nFound {len(balloon_annotations)} balloon annotations in sample file\")\n",
    "                print(\"Sample balloon annotation:\", balloon_annotations[0])\n",
    "            else:\n",
    "                print(\"\\nWarning: No balloon annotations found in sample file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dd34a61a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T05:37:11.891319Z",
     "iopub.status.busy": "2025-10-06T05:37:11.890758Z",
     "iopub.status.idle": "2025-10-06T05:37:11.912417Z",
     "shell.execute_reply": "2025-10-06T05:37:11.911776Z",
     "shell.execute_reply.started": "2025-10-06T05:37:11.891295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 2. Data Preparation ---\n",
    "\n",
    "def decode_rle(rle):\n",
    "    \"\"\"Decode RLE format to binary mask\"\"\"\n",
    "    if isinstance(rle, dict):\n",
    "        # COCO RLE format\n",
    "        h, w = rle['size']\n",
    "        if 'counts' in rle:\n",
    "            counts = rle['counts']\n",
    "            if isinstance(counts, str):\n",
    "                # If counts is a string, it's compressed RLE\n",
    "                from pycocotools import mask as mask_utils\n",
    "                import numpy as np\n",
    "                try:\n",
    "                    # Convert RLE to binary mask\n",
    "                    mask = mask_utils.decode(rle)\n",
    "                    return mask.astype(np.float32)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error decoding RLE: {str(e)}\")\n",
    "                    return None\n",
    "    return None\n",
    "\n",
    "def polygon_to_mask(segmentation, img_height, img_width):\n",
    "    \"\"\"Convert segmentation to binary mask, handling both polygon and RLE formats\"\"\"\n",
    "    import numpy as np\n",
    "    from PIL import Image, ImageDraw\n",
    "    \n",
    "    # Check if segmentation is RLE format\n",
    "    if isinstance(segmentation, dict) and 'counts' in segmentation and 'size' in segmentation:\n",
    "        return decode_rle(segmentation)\n",
    "    \n",
    "    # If not RLE, treat as polygon\n",
    "    mask = Image.new('L', (img_width, img_height), 0)\n",
    "    \n",
    "    # Handle case where segmentation is a list of polygons\n",
    "    if isinstance(segmentation, list):\n",
    "        # If first element is also a list, we have multiple polygons\n",
    "        if segmentation and isinstance(segmentation[0], list):\n",
    "            polygons = segmentation\n",
    "        else:\n",
    "            # Single polygon as flat list of coordinates\n",
    "            polygons = [segmentation]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        for polygon in polygons:\n",
    "            if len(polygon) >= 6:  # At least 3 points (6 coordinates)\n",
    "                # Convert flat list to list of points\n",
    "                points = []\n",
    "                for i in range(0, len(polygon), 2):\n",
    "                    x, y = polygon[i], polygon[i + 1]\n",
    "                    if not (isinstance(x, (int, float)) and isinstance(y, (int, float))):\n",
    "                        continue\n",
    "                    if x < 0 or y < 0 or x > img_width or y > img_height:\n",
    "                        continue\n",
    "                    points.append((x, y))\n",
    "                \n",
    "                if len(points) >= 3:  # Need at least 3 points for a polygon\n",
    "                    # Draw the polygon\n",
    "                    ImageDraw.Draw(mask).polygon(points, outline=1, fill=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating mask: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    mask = np.array(mask, dtype=np.float32)\n",
    "    return mask\n",
    "\n",
    "def xywh_to_yolo(bbox, img_width, img_height):\n",
    "    \"\"\"Convert XYWH bbox to YOLO format (x_center, y_center, width, height) normalized\"\"\"\n",
    "    try:\n",
    "        # Handle different bbox formats\n",
    "        if isinstance(bbox, (list, tuple)):\n",
    "            if len(bbox) == 4:\n",
    "                x, y, w, h = bbox\n",
    "            else:\n",
    "                raise ValueError(\"Bbox must have 4 coordinates\")\n",
    "        elif isinstance(bbox, dict):\n",
    "            x = bbox.get('x')\n",
    "            y = bbox.get('y')\n",
    "            w = bbox.get('width')\n",
    "            h = bbox.get('height')\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported bbox format\")\n",
    "        \n",
    "        # Validate values\n",
    "        if any(not isinstance(v, (int, float)) for v in [x, y, w, h]):\n",
    "            raise ValueError(\"Bbox coordinates must be numeric\")\n",
    "        if img_width <= 0 or img_height <= 0:\n",
    "            raise ValueError(\"Image dimensions must be positive\")\n",
    "        if w <= 0 or h <= 0:\n",
    "            raise ValueError(\"Bbox width and height must be positive\")\n",
    "        if x < 0 or y < 0 or x + w > img_width or y + h > img_height:\n",
    "            raise ValueError(\"Bbox coordinates out of image bounds\")\n",
    "        \n",
    "        return [\n",
    "            (x + w/2) / img_width,  # x_center\n",
    "            (y + h/2) / img_height, # y_center\n",
    "            w / img_width,          # width\n",
    "            h / img_height          # height\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting bbox: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def find_image_file(image_root, file_name):\n",
    "    \"\"\"Find image file with case-insensitive matching\"\"\"\n",
    "    try:\n",
    "        # Get manga directory and image name\n",
    "        parts = file_name.split('/')\n",
    "        if len(parts) == 2:\n",
    "            manga_dir, image_name = parts\n",
    "        else:\n",
    "            manga_dir = os.path.dirname(file_name)\n",
    "            image_name = os.path.basename(file_name)\n",
    "        \n",
    "        # First try exact path\n",
    "        direct_path = os.path.join(image_root, manga_dir, image_name)\n",
    "        if os.path.exists(direct_path):\n",
    "            return os.path.join(manga_dir, image_name)\n",
    "        \n",
    "        # Try case-insensitive search\n",
    "        manga_path = None\n",
    "        for root_dir in os.listdir(image_root):\n",
    "            if root_dir.lower() == manga_dir.lower():\n",
    "                manga_path = os.path.join(image_root, root_dir)\n",
    "                break\n",
    "        \n",
    "        if manga_path and os.path.exists(manga_path):\n",
    "            for img_file in os.listdir(manga_path):\n",
    "                if img_file.lower() == image_name.lower():\n",
    "                    return os.path.join(os.path.basename(manga_path), img_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding image file: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "def prepare_manga_balloon_data(json_dir, image_root):\n",
    "    \"\"\"\n",
    "    Loads and processes JSON files in COCO format, filtering for balloon annotations.\n",
    "    Uses sequential processing to avoid multiprocessing issues.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Starting Data Preparation ===\")\n",
    "    print(f\"JSON directory: {json_dir}\")\n",
    "    print(f\"Image root directory: {image_root}\")\n",
    "    print(f\"Target category ID: {TARGET_CATEGORY_ID}\")\n",
    "    print(f\"Target category name: {TARGET_CATEGORY_NAME}\")\n",
    "    \n",
    "    # First, install pycocotools if not already installed\n",
    "    try:\n",
    "        import pycocotools\n",
    "    except ImportError:\n",
    "        print(\"Installing pycocotools...\")\n",
    "        os.system('pip install pycocotools')\n",
    "        import pycocotools\n",
    "    \n",
    "    dataset_dicts = {}\n",
    "    json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]\n",
    "    print(f\"\\nFound {len(json_files)} JSON files\")\n",
    "    \n",
    "    total_images = 0\n",
    "    total_annotations = 0\n",
    "    skipped_annotations = 0\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=\"Processing JSON files\"):\n",
    "        try:\n",
    "            with open(os.path.join(json_dir, json_file), 'r', encoding='utf-8') as f:\n",
    "                manga_data = json.load(f)\n",
    "                \n",
    "                # Validate JSON structure\n",
    "                if not isinstance(manga_data, dict):\n",
    "                    print(f\"Warning: {json_file} has invalid format\")\n",
    "                    continue\n",
    "                \n",
    "                if not all(k in manga_data for k in ['images', 'annotations']):\n",
    "                    print(f\"Warning: {json_file} missing required keys\")\n",
    "                    continue\n",
    "                \n",
    "                # Create lookup for images\n",
    "                image_lookup = {}\n",
    "                for img in manga_data['images']:\n",
    "                    if 'id' not in img or 'file_name' not in img:\n",
    "                        continue\n",
    "                    image_lookup[img['id']] = img\n",
    "                \n",
    "                # Process annotations\n",
    "                file_annotations = 0\n",
    "                for ann in manga_data['annotations']:\n",
    "                    try:\n",
    "                        # Check if this is a balloon annotation\n",
    "                        if ann.get('category_id') != TARGET_CATEGORY_ID:\n",
    "                            continue\n",
    "                        \n",
    "                        # Get image info\n",
    "                        img_id = ann.get('image_id')\n",
    "                        if img_id not in image_lookup:\n",
    "                            continue\n",
    "                        \n",
    "                        img_info = image_lookup[img_id]\n",
    "                        relative_path = find_image_file(image_root, img_info['file_name'])\n",
    "                        \n",
    "                        if not relative_path:\n",
    "                            skipped_annotations += 1\n",
    "                            continue\n",
    "                        \n",
    "                        img_path = os.path.join(image_root, relative_path)\n",
    "                        if not os.path.exists(img_path):\n",
    "                            skipped_annotations += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Convert bbox to YOLO format\n",
    "                        bbox = ann.get('bbox')\n",
    "                        if not bbox:\n",
    "                            continue\n",
    "                            \n",
    "                        yolo_bbox = xywh_to_yolo(\n",
    "                            bbox,\n",
    "                            img_info['width'],\n",
    "                            img_info['height']\n",
    "                        )\n",
    "                        \n",
    "                        if not yolo_bbox:\n",
    "                            skipped_annotations += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Convert segmentation to mask\n",
    "                        segmentation = ann.get('segmentation')\n",
    "                        if not segmentation:\n",
    "                            continue\n",
    "                            \n",
    "                        mask = polygon_to_mask(\n",
    "                            segmentation,\n",
    "                            img_info['height'],\n",
    "                            img_info['width']\n",
    "                        )\n",
    "                        \n",
    "                        if mask is None:\n",
    "                            skipped_annotations += 1\n",
    "                            continue\n",
    "                        \n",
    "                        # Add to dataset\n",
    "                        if img_path not in dataset_dicts:\n",
    "                            dataset_dicts[img_path] = {\n",
    "                                'image_id': img_id,\n",
    "                                'width': img_info['width'],\n",
    "                                'height': img_info['height'],\n",
    "                                'bboxes': [],\n",
    "                                'masks': []\n",
    "                            }\n",
    "                            total_images += 1\n",
    "                        \n",
    "                        dataset_dicts[img_path]['bboxes'].append(yolo_bbox)\n",
    "                        dataset_dicts[img_path]['masks'].append(mask)\n",
    "                        total_annotations += 1\n",
    "                        file_annotations += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing annotation: {str(e)}\")\n",
    "                        continue\n",
    "                        \n",
    "                print(f\"Processed {json_file}: Found {file_annotations} valid balloon annotations\")\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding {json_file}: {str(e)}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n=== Data Processing Summary ===\")\n",
    "    print(f\"Total images with valid annotations: {total_images}\")\n",
    "    print(f\"Total valid annotations: {total_annotations}\")\n",
    "    print(f\"Skipped annotations: {skipped_annotations}\")\n",
    "    \n",
    "    if not dataset_dicts:\n",
    "        print(\"\\nERROR: No valid data found after processing!\")\n",
    "        print(\"Please check the following:\")\n",
    "        print(f\"1. Target category ID ({TARGET_CATEGORY_ID}) exists in the dataset\")\n",
    "        print(\"2. Image files exist in the correct structure\")\n",
    "        print(\"3. Annotations have valid segmentation data\")\n",
    "        raise ValueError(\"No valid data found after processing\")\n",
    "    \n",
    "    print(f\"\\nFinished data preparation. Found {len(dataset_dicts)} images containing '{TARGET_CATEGORY_NAME}'.\")\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4fac6330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T05:37:11.913444Z",
     "iopub.status.busy": "2025-10-06T05:37:11.913197Z",
     "iopub.status.idle": "2025-10-06T05:37:31.622014Z",
     "shell.execute_reply": "2025-10-06T05:37:31.621064Z",
     "shell.execute_reply.started": "2025-10-06T05:37:11.913417Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 109 JSON files using 4 cores...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b267e7f7bc441eaccdbc0b449bc357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing JSON files:   0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10619 images and 130180 annotations\n",
      "\n",
      "Converting annotations to YOLO format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95fd038292da4b1f87a537333a073f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting annotations:   0%|          | 0/10619 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data preparation. Found 9916 images containing 'balloon'.\n",
      "\n",
      "Grouping data by manga series for a robust train/val split...\n",
      "Found 109 unique manga series.\n",
      "Splitting into 87 series for training and 22 for validation.\n",
      "\n",
      "Processing training split...\n",
      "Processing validation split...\n",
      "\n",
      "Dataset created successfully:\n",
      "Training images: 8690\n",
      "Validation images: 2687\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Split and Prepare YOLO Dataset ---\n",
    "\n",
    "# Prepare the data\n",
    "all_data = prepare_manga_balloon_data(JSON_DIR, IMAGE_ROOT_DIR)\n",
    "\n",
    "# --- Group data by manga title ---\n",
    "print(\"\\nGrouping data by manga series for a robust train/val split...\")\n",
    "grouped_data = defaultdict(dict)\n",
    "for img_path, data in all_data.items():\n",
    "    # Extract manga name from the file path\n",
    "    manga_name = Path(img_path).parent.name\n",
    "    grouped_data[manga_name][img_path] = data\n",
    "\n",
    "print(f\"Found {len(grouped_data)} unique manga series.\")\n",
    "\n",
    "# --- Split manga titles, not individual pages ---\n",
    "manga_titles = list(grouped_data.keys())\n",
    "train_titles, val_titles = train_test_split(manga_titles, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Splitting into {len(train_titles)} series for training and {len(val_titles)} for validation.\")\n",
    "\n",
    "# Function to copy images and create label/mask files\n",
    "def process_dataset_split(titles, split_type):\n",
    "    total_masks = 0\n",
    "    for manga_title in titles:\n",
    "        for img_path, data in grouped_data[manga_title].items():\n",
    "            # Generate unique identifier for the image\n",
    "            img_identifier = f\"{manga_title}_{Path(img_path).stem}\"\n",
    "            \n",
    "            # Copy image\n",
    "            dest_img_path = os.path.join(DATASET_DIR, f'images/{split_type}', \n",
    "                                       f\"{img_identifier}.jpg\")\n",
    "            shutil.copy2(img_path, dest_img_path)\n",
    "            \n",
    "            # Create label file (for bounding boxes)\n",
    "            label_path = os.path.join(DATASET_DIR, f'labels/{split_type}', f\"{img_identifier}.txt\")\n",
    "            \n",
    "            # Create masks directory for this image\n",
    "            masks_dir = os.path.join(DATASET_DIR, f'masks/{split_type}', img_identifier)\n",
    "            os.makedirs(masks_dir, exist_ok=True)\n",
    "            \n",
    "            # Write YOLO format labels and save masks\n",
    "            with open(label_path, 'w') as f:\n",
    "                for idx, (bbox, mask) in enumerate(zip(data['bboxes'], data['masks'])):\n",
    "                    # Write bbox in YOLO format\n",
    "                    f.write(f\"0 {' '.join(map(str, bbox))}\\n\")\n",
    "                    \n",
    "                    # Save mask as PNG file (better for binary masks)\n",
    "                    mask_path = os.path.join(masks_dir, f'{idx}.png')\n",
    "                    cv2.imwrite(mask_path, (mask * 255).astype(np.uint8))\n",
    "                    total_masks += 1\n",
    "    \n",
    "    return total_masks\n",
    "\n",
    "# Process train and validation splits\n",
    "print(\"\\nProcessing training split...\")\n",
    "train_masks = process_dataset_split(train_titles, 'train')\n",
    "print(\"Processing validation split...\")\n",
    "val_masks = process_dataset_split(val_titles, 'val')\n",
    "\n",
    "# Verify split\n",
    "train_images = len(list(Path(DATASET_DIR).glob('images/train/*.jpg')))\n",
    "val_images = len(list(Path(DATASET_DIR).glob('images/val/*.jpg')))\n",
    "\n",
    "# Count masks (using proper path pattern)\n",
    "train_mask_files = sum(len(list(Path(mask_dir).glob('*.png'))) \n",
    "                      for mask_dir in Path(DATASET_DIR).glob('masks/train/*'))\n",
    "val_mask_files = sum(len(list(Path(mask_dir).glob('*.png'))) \n",
    "                    for mask_dir in Path(DATASET_DIR).glob('masks/val/*'))\n",
    "\n",
    "print(f\"\\nDataset created successfully:\")\n",
    "print(f\"Training images: {train_images}\")\n",
    "print(f\"Training masks: {train_mask_files}\")\n",
    "print(f\"Validation images: {val_images}\")\n",
    "print(f\"Validation masks: {val_mask_files}\")\n",
    "\n",
    "# Additional verification\n",
    "print(\"\\nVerifying mask creation:\")\n",
    "train_mask_dirs = list(Path(DATASET_DIR).glob('masks/train/*'))\n",
    "val_mask_dirs = list(Path(DATASET_DIR).glob('masks/val/*'))\n",
    "print(f\"Number of training image directories with masks: {len(train_mask_dirs)}\")\n",
    "print(f\"Number of validation image directories with masks: {len(val_mask_dirs)}\")\n",
    "\n",
    "if train_mask_files == 0 or val_mask_files == 0:\n",
    "    print(\"\\nWARNING: No masks were created! Checking first few images for debugging:\")\n",
    "    sample_images = list(Path(DATASET_DIR).glob('images/train/*.jpg'))[:5]\n",
    "    for img_path in sample_images:\n",
    "        img_id = img_path.stem\n",
    "        mask_dir = Path(DATASET_DIR) / 'masks/train' / img_id\n",
    "        print(f\"\\nChecking {img_id}:\")\n",
    "        print(f\"- Image exists: {img_path.exists()}\")\n",
    "        print(f\"- Mask directory exists: {mask_dir.exists()}\")\n",
    "        if mask_dir.exists():\n",
    "            print(f\"- Number of masks: {len(list(mask_dir.glob('*.png')))}\")\n",
    "            print(f\"- Mask directory contents: {list(mask_dir.glob('*'))}\")\n",
    "\n",
    "# Save split information\n",
    "split_info = pd.DataFrame([\n",
    "    {'manga_title': title, 'dataset_split': 'train'} for title in train_titles\n",
    "] + [\n",
    "    {'manga_title': title, 'dataset_split': 'validation'} for title in val_titles\n",
    "])\n",
    "split_info.sort_values('manga_title').to_csv('manga_split_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5ff75758",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T05:37:31.623458Z",
     "iopub.status.busy": "2025-10-06T05:37:31.622955Z",
     "iopub.status.idle": "2025-10-06T05:37:31.634358Z",
     "shell.execute_reply": "2025-10-06T05:37:31.633653Z",
     "shell.execute_reply.started": "2025-10-06T05:37:31.623433Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Train/Validation Split Summary ---\n",
      "Manga Series Split Distribution:\n",
      "                          manga_title dataset_split\n",
      "0                                ARMS    validation\n",
      "1                  AisazuNihaIrarenai         train\n",
      "2                    AkkeraKanjinchou         train\n",
      "3                             Akuhamu         train\n",
      "4                        AosugiruHaru    validation\n",
      "5                       AppareKappore         train\n",
      "6                               Arisa         train\n",
      "7                           BEMADER_P         train\n",
      "8                 BakuretsuKungFuGirl         train\n",
      "9                            Belmondo         train\n",
      "10                  BokuHaSitatakaKun    validation\n",
      "11            BurariTessenTorimonocho    validation\n",
      "12                        ByebyeC-BOY    validation\n",
      "13                Count3DeKimeteAgeru         train\n",
      "14                            DollGun         train\n",
      "15                       Donburakokko         train\n",
      "16                        DualJustice         train\n",
      "17                         EienNoWith         train\n",
      "18                            EvaLady    validation\n",
      "19                EverydayOsakanaChan         train\n",
      "20                     GOOD_KISS_Ver2         train\n",
      "21                        GakuenNoise         train\n",
      "22                    GarakutayaManta         train\n",
      "23                       GinNoChimera         train\n",
      "24                             Hamlet         train\n",
      "25   HanzaiKousyouninMinegishiEitarou         train\n",
      "26              HaruichibanNoFukukoro         train\n",
      "27                      HarukaRefrain         train\n",
      "28                      HealingPlanet         train\n",
      "29                        HeiseiJimen         train\n",
      "30          HighschoolKimengumi_vol01    validation\n",
      "31          HighschoolKimengumi_vol20         train\n",
      "32                     HinagikuKenzan         train\n",
      "33                      HisokaReturns         train\n",
      "34                      JangiriPonpon         train\n",
      "35                      JijiBabaFight         train\n",
      "36                            Joouari    validation\n",
      "37                          Jyovolley         train\n",
      "38                  KarappoHighschool         train\n",
      "39               KimiHaBokuNoTaiyouDa         train\n",
      "40                  KoukouNoHitotachi         train\n",
      "41                       KuroidoGanka         train\n",
      "42                    KyokugenCyclone    validation\n",
      "43               LancelotFullThrottle         train\n",
      "44                     LoveHina_vol01         train\n",
      "45                     LoveHina_vol14    validation\n",
      "46                          MAD_STONE         train\n",
      "47                         MadouTaiga         train\n",
      "48                    MagicStarGakuin         train\n",
      "49                       MagicianLoad         train\n",
      "50                MariaSamaNihaNaisyo         train\n",
      "51                    MayaNoAkaiKutsu         train\n",
      "52                       MemorySeijin         train\n",
      "53                 MeteoSanStrikeDesu         train\n",
      "54                           MiraiSan         train\n",
      "55                   MisutenaideDaisy         train\n",
      "56                  MoeruOnisan_vol01    validation\n",
      "57                  MoeruOnisan_vol19         train\n",
      "58                  MomoyamaHaikagura         train\n",
      "59                  MukoukizuNoChonbo         train\n",
      "60                MutekiBoukenSyakuma         train\n",
      "61                           Nekodama         train\n",
      "62                       NichijouSoup         train\n",
      "63                         Ningyoushi         train\n",
      "64                           OL_Lunch    validation\n",
      "65             OhWareraRettouSeitokai         train\n",
      "66                            PLANET7         train\n",
      "67                        ParaisoRoad         train\n",
      "68                    PikaruGenkiDesu    validation\n",
      "69                     PlatinumJungle         train\n",
      "70                  PrayerHaNemurenai    validation\n",
      "71                         PrismHeart         train\n",
      "72                        PsychoStaff    validation\n",
      "73                            Raphael         train\n",
      "74                        ReveryEarth         train\n",
      "75               RinToSiteSippuNoNaka         train\n",
      "76                         RisingGirl         train\n",
      "77                            Saisoku         train\n",
      "78                    SaladDays_vol01    validation\n",
      "79                    SaladDays_vol18    validation\n",
      "80           SamayoeruSyonenNiJunaiWo    validation\n",
      "81                     SeisinkiVulnus    validation\n",
      "82               ShimatteIkouze_vol01         train\n",
      "83               ShimatteIkouze_vol26         train\n",
      "84                        SonokiDeABC    validation\n",
      "85                    SyabondamaKieta         train\n",
      "86                      TaiyouNiSmash         train\n",
      "87                TapkunNoTanteisitsu         train\n",
      "88                    TasogareTsushin         train\n",
      "89                      TennenSenshiG         train\n",
      "90         TensiNoHaneToAkumaNoShippo         train\n",
      "91                           TetsuSan         train\n",
      "92                        ThatIzumiko         train\n",
      "93                      TotteokiNoABC         train\n",
      "94                     ToutaMairimasu         train\n",
      "95                        TouyouKidan         train\n",
      "96                     TsubasaNoKioku    validation\n",
      "97                    UchiNoNyanDiary         train\n",
      "98                     UchuKigekiM774         train\n",
      "99                        UltraEleven         train\n",
      "100                    UnbalanceTokyo         train\n",
      "101                WarewareHaOniDearu         train\n",
      "102                      YamatoNoHane         train\n",
      "103                      YasasiiAkuma         train\n",
      "104                 YouchienBoueigumi         train\n",
      "105                       YoumaKourin         train\n",
      "106                   YukiNoFuruMachi         train\n",
      "107                     YumeNoKayoiji    validation\n",
      "108                    YumeiroCooking         train\n",
      "\n",
      "Split summary has been saved to 'manga_split_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# --- Create, Display, and Save Split Summary ---\n",
    "# ===================================================================\n",
    "print(\"\\n--- Generating Train/Validation Split Summary ---\")\n",
    "\n",
    "# Create a list of dictionaries for the DataFrame\n",
    "train_split_info = [{'manga_title': title, 'dataset_split': 'train'} for title in train_titles]\n",
    "val_split_info = [{'manga_title': title, 'dataset_split': 'validation'} for title in val_titles]\n",
    "\n",
    "# Combine and create the DataFrame\n",
    "split_summary_df = pd.DataFrame(train_split_info + val_split_info)\n",
    "split_summary_df = split_summary_df.sort_values(by='manga_title').reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame in the notebook output\n",
    "print(\"Manga Series Split Distribution:\")\n",
    "print(split_summary_df.to_string()) # .to_string() ensures all rows are printed\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = \"manga_split_summary.csv\"\n",
    "split_summary_df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nSplit summary has been saved to '{csv_filename}'\")\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "90afe908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T05:37:31.635273Z",
     "iopub.status.busy": "2025-10-06T05:37:31.635061Z",
     "iopub.status.idle": "2025-10-06T05:37:34.863020Z",
     "shell.execute_reply": "2025-10-06T05:37:34.862385Z",
     "shell.execute_reply.started": "2025-10-06T05:37:31.635257Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Configuration ===\n",
      "\n",
      "YAML files saved to:\n",
      "1. Output directory: output/dataset.yaml\n",
      "2. Dataset directory: balloon_dataset/dataset.yaml\n",
      "\n",
      "Dataset configuration:\n",
      "path: /kaggle/working/balloon_dataset\n",
      "train: images/train\n",
      "val: images/val\n",
      "test: images/val\n",
      "names:\n",
      "  0: balloon\n",
      "task: detect\n",
      "nc: 1\n",
      "width: 1280\n",
      "height: 1280\n",
      "batch: 4\n",
      "epochs: 100\n",
      "device: 0\n",
      "\n",
      "\n",
      "Verified: output/dataset.yaml exists\n",
      "File size: 186 bytes\n",
      "\n",
      "Verified: balloon_dataset/dataset.yaml exists\n",
      "File size: 186 bytes\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Create YAML Configuration ---\n",
    "\n",
    "# Tạo thư mục output nếu chưa tồn tại\n",
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define dataset configuration\n",
    "dataset_config = {\n",
    "    # Đường dẫn dataset\n",
    "    'path': os.path.abspath(DATASET_DIR),  # Đường dẫn tuyệt đối đến thư mục dataset\n",
    "    'train': os.path.join('images', 'train'),  # Thư mục ảnh training\n",
    "    'val': os.path.join('images', 'val'),      # Thư mục ảnh validation\n",
    "    'test': os.path.join('images', 'val'),     # Sử dụng validation set làm test set\n",
    "    \n",
    "    # Thông tin classes\n",
    "    'names': {\n",
    "        0: TARGET_CATEGORY_NAME  # Class 0: balloon\n",
    "    },\n",
    "    \n",
    "    # Thông số task\n",
    "    'task': 'segment',          # Task: instance segmentation\n",
    "    'nc': 1,                    # Số lượng class: 1 (balloon)\n",
    "    \n",
    "    # Thông tin thêm\n",
    "    'width': 512,               # Giảm kích thước ảnh để tiết kiệm bộ nhớ\n",
    "    'height': 512,              # Giữ tỷ lệ 1:1\n",
    "    'batch': 1,                 # Batch size nhỏ nhất cho segmentation\n",
    "    'epochs': 100,              # Số epochs khuyến nghị\n",
    "    'device': [0, 1],           # Sử dụng cả 2 GPU\n",
    "}\n",
    "\n",
    "# Save configuration vào thư mục output\n",
    "yaml_path = os.path.join(output_dir, 'dataset.yaml')\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(dataset_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "# Save một bản copy trong thư mục dataset\n",
    "dataset_yaml_path = os.path.join(DATASET_DIR, 'dataset.yaml')\n",
    "with open(dataset_yaml_path, 'w') as f:\n",
    "    yaml.dump(dataset_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(\"=== Dataset Configuration ===\")\n",
    "print(f\"\\nYAML files saved to:\")\n",
    "print(f\"1. Output directory: {yaml_path}\")\n",
    "print(f\"2. Dataset directory: {dataset_yaml_path}\")\n",
    "print(\"\\nDataset configuration:\")\n",
    "print(yaml.dump(dataset_config, default_flow_style=False, sort_keys=False))\n",
    "\n",
    "# Verify the files exist\n",
    "for path in [yaml_path, dataset_yaml_path]:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"\\nVerified: {path} exists\")\n",
    "        print(f\"File size: {os.path.getsize(path)} bytes\")\n",
    "    else:\n",
    "        print(f\"\\nWarning: {path} was not created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1970d3e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T05:37:34.865509Z",
     "iopub.status.busy": "2025-10-06T05:37:34.865285Z",
     "iopub.status.idle": "2025-10-06T05:37:36.129035Z",
     "shell.execute_reply": "2025-10-06T05:37:36.128336Z",
     "shell.execute_reply.started": "2025-10-06T05:37:34.865493Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU Configuration ===\n",
      "Using GPU: Tesla T4\n",
      "- Memory: 14.7 GB\n",
      "- CUDA Capability: 7.5\n",
      "\n",
      "=== Model Initialization ===\n",
      "Loading YOLOv8 model...\n",
      "\n",
      "Using configuration from: output/dataset.yaml\n",
      "\n",
      "=== Training Configuration ===\n",
      "\n",
      "Dataset:\n",
      "  data: output/dataset.yaml\n",
      "\n",
      "Training:\n",
      "  epochs: 100\n",
      "  imgsz: 1280\n",
      "  batch: 4\n",
      "  patience: 15\n",
      "\n",
      "Optimization:\n",
      "  lr0: 0.001\n",
      "  lrf: 0.01\n",
      "  weight_decay: 0.0005\n",
      "  warmup_epochs: 3.0\n",
      "  cos_lr: True\n",
      "\n",
      "Augmentation:\n",
      "  fliplr: 0.5\n",
      "  mosaic: 1.0\n",
      "  mixup: 0.2\n",
      "  copy_paste: 0.1\n",
      "  scale: 0.3\n",
      "\n",
      "GPU Config:\n",
      "  device: 1\n",
      "  workers: 8\n",
      "  amp: True\n",
      "\n",
      "Output:\n",
      "  project: output/runs/detect\n",
      "  name: balloon_yolov8\n",
      "\n",
      "NOTES:\n",
      "- Using single GPU with batch size 4\n",
      "- Gradient accumulation steps: 16\n",
      "- Auto mixed precision (AMP) enabled for faster training\n",
      "- Early stopping will pause training if no improvement in 15 epochs\n",
      "- Cosine learning rate scheduler enabled\n",
      "- Using 8 worker processes for data loading\n",
      "- All outputs will be saved to: output/runs/detect\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Initialize and Train YOLOv8 Model ---\n",
    "\n",
    "# Kiểm tra GPU và clear cache\n",
    "print(\"=== GPU Configuration ===\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"Using GPU: {props.name}\")\n",
    "    print(f\"- Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"- CUDA Capability: {props.major}.{props.minor}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU found. Training on CPU will be very slow!\")\n",
    "\n",
    "# Initialize YOLOv8 model\n",
    "print(\"\\n=== Model Initialization ===\")\n",
    "print(\"Loading YOLOv8-seg model...\")\n",
    "model = YOLO('yolov8s-seg.pt')  # Load segmentation model\n",
    "\n",
    "# Calculate optimal batch size for single GPU\n",
    "BATCH_SIZE = 1  # Reduced batch size for segmentation\n",
    "ACCUMULATE = max(1, round(64 / BATCH_SIZE))  # Accumulate gradients\n",
    "\n",
    "# Verify YAML file exists\n",
    "if not os.path.exists(yaml_path):\n",
    "    raise FileNotFoundError(f\"YAML configuration file not found at {yaml_path}\")\n",
    "print(f\"\\nUsing configuration from: {yaml_path}\")\n",
    "\n",
    "# Cấu hình training\n",
    "train_args = {\n",
    "    # [1] Cấu hình dataset\n",
    "    'data': yaml_path,         # File cấu hình dataset\n",
    "    \n",
    "    # [2] Các tham số training chính\n",
    "    'epochs': 100,            # Số epochs training\n",
    "    'imgsz': 512,            # Giảm kích thước ảnh\n",
    "    'batch': BATCH_SIZE,     # Batch size nhỏ hơn\n",
    "    'patience': 15,          # Early stopping nếu không cải thiện\n",
    "    \n",
    "    # [3] Tối ưu hóa\n",
    "    'lr0': 0.0001,          # Giảm learning rate\n",
    "    'lrf': 0.01,            # Learning rate cuối\n",
    "    'weight_decay': 0.0005, # Chống overfitting\n",
    "    'warmup_epochs': 3.0,   # Epochs warmup\n",
    "    'warmup_momentum': 0.8, # Momentum during warmup\n",
    "    'warmup_bias_lr': 0.1,  # Warmup initial bias lr\n",
    "    'box': 7.5,            # Box loss gain\n",
    "    'cls': 0.5,            # Cls loss gain\n",
    "    'mask_ratio': 4,       # Mask loss gain\n",
    "    'overlap': True,       # Overlap masks for segmentation\n",
    "    \n",
    "    # [4] Data augmentation (giảm bớt để tiết kiệm bộ nhớ)\n",
    "    'fliplr': 0.5,          # Lật ngang\n",
    "    'flipud': 0.0,          # Không lật dọc\n",
    "    'mosaic': 0.0,          # Tắt mosaic do cần nhiều bộ nhớ\n",
    "    'mixup': 0.0,           # Tắt mixup\n",
    "    'copy_paste': 0.0,      # Tắt copy-paste\n",
    "    'degrees': 0.0,         # Image rotation\n",
    "    'translate': 0.1,       # Giảm translation\n",
    "    'scale': 0.2,          # Giảm scale\n",
    "    'shear': 0.0,          # Image shear\n",
    "    \n",
    "    # [5] DataLoader configuration\n",
    "    'workers': 0,           # Disable multiprocessing for DataLoader\n",
    "    'pin_memory': False,    # Disable pin_memory\n",
    "    'persistent_workers': False,  # Disable persistent workers\n",
    "    'rect': False,         # Tắt rectangular training cho segmentation\n",
    "    'cos_lr': True,        # Cosine learning rate\n",
    "    'amp': True,           # Auto mixed precision\n",
    "    'multi_scale': False,  # Tắt multi-scale training\n",
    "    \n",
    "    # [6] Thư mục và checkpoint\n",
    "    'project': os.path.join(output_dir, 'runs/segment'),\n",
    "    'name': 'balloon_yolov8seg',\n",
    "    'exist_ok': True,\n",
    "    'pretrained': True,\n",
    "    \n",
    "    # [7] Memory optimization\n",
    "    'cache': False,        # Không cache images\n",
    "    'image_weights': False,# Không dùng image weights\n",
    "    'save_period': -1,    # Chỉ lưu best model\n",
    "    'val': True,          # Validate mỗi epoch\n",
    "    'save': True,         # Lưu checkpoints\n",
    "    \n",
    "    # [8] Segmentation specific\n",
    "    'mask': True,         # Enable mask training\n",
    "    'retina_masks': True, # Use high-quality masks\n",
    "    'overlap_mask': True, # Allow mask overlap\n",
    "    'mask_ratio': 4,      # Mask loss ratio\n",
    "    'dropout': 0.2,       # Dropout rate\n",
    "}\n",
    "\n",
    "print(\"\\n=== Training Configuration ===\")\n",
    "sections = {\n",
    "    'Dataset': ['data'],\n",
    "    'Training': ['epochs', 'imgsz', 'batch', 'patience'],\n",
    "    'Optimization': ['lr0', 'lrf', 'weight_decay', 'warmup_epochs', 'cos_lr'],\n",
    "    'Augmentation': ['fliplr', 'mosaic', 'mixup', 'copy_paste', 'scale'],\n",
    "    'GPU Config': ['device', 'workers', 'amp'],\n",
    "    'Segmentation': ['mask', 'retina_masks', 'overlap_mask', 'mask_ratio'],\n",
    "    'Output': ['project', 'name']\n",
    "}\n",
    "\n",
    "for section, params in sections.items():\n",
    "    print(f\"\\n{section}:\")\n",
    "    for p in params:\n",
    "        if p in train_args:\n",
    "            print(f\"  {p}: {train_args[p]}\")\n",
    "\n",
    "print(\"\\nNOTES:\")\n",
    "print(f\"- Using single GPU with minimal batch size {BATCH_SIZE}\")\n",
    "print(f\"- Gradient accumulation steps: {ACCUMULATE}\")\n",
    "print(\"- Auto mixed precision (AMP) enabled for memory efficiency\")\n",
    "print(\"- Disabled multiprocessing in DataLoader\")\n",
    "print(\"- Disabled heavy augmentations to save memory\")\n",
    "print(\"- High-quality segmentation masks enabled\")\n",
    "print(f\"- All outputs will be saved to: {train_args['project']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973bd00-d50c-4f62-91be-0cdc6c333409",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6f40a8c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T05:37:36.129954Z",
     "iopub.status.busy": "2025-10-06T05:37:36.129744Z",
     "iopub.status.idle": "2025-10-06T05:37:37.966389Z",
     "shell.execute_reply": "2025-10-06T05:37:37.965275Z",
     "shell.execute_reply.started": "2025-10-06T05:37:36.129937Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Setup ===\n",
      "Training on GPU: Tesla T4\n",
      "Batch size: 4\n",
      "\n",
      "Training will run for 100 epochs\n",
      "Early stopping patience: 15 epochs\n",
      "Results will be saved to: output/runs/detect/balloon_yolov8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd661d70f1944e3b705dcb3358eca1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.205 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:1 (Tesla T4, 15095MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.1, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=output/dataset.yaml, degrees=0.0, deterministic=True, device=1, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1280, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.2, mode=train, model=yolov8s-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=balloon_yolov8, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=15, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=output/runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/output/runs/detect/balloon_yolov8, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.3, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.2, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2770931  ultralytics.nn.modules.head.Segment          [1, 32, 128, [128, 256, 512]] \n",
      "YOLOv8s-seg summary: 151 layers, 11,790,483 parameters, 11,790,467 gradients, 40.2 GFLOPs\n",
      "\n",
      "Transferred 411/417 items from pretrained weights\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 3196 has 14.73 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 80.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/1360799182.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nGPU Memory Used: {mem_used:.0f}MB / {mem_total:.1f}GB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     results = model.train(\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_ddp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_setup_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;34m\"\"\"Build dataloaders and optimizer on correct rank process.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mBaseModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAn\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mBaseModel\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Detect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         if isinstance(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 3196 has 14.73 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 80.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# --- Start Training ---\n",
    "\n",
    "print(\"=== Training Setup ===\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Training on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Batch size: {train_args['batch']}\")\n",
    "else:\n",
    "    print(\"Training on CPU (not recommended)\")\n",
    "    print(f\"Batch size: {train_args['batch']}\")\n",
    "\n",
    "print(f\"\\nTraining will run for {train_args['epochs']} epochs\")\n",
    "print(f\"Early stopping patience: {train_args['patience']} epochs\")\n",
    "print(f\"Results will be saved to: {os.path.join(train_args['project'], train_args['name'])}\")\n",
    "\n",
    "# Training with progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "with tqdm(total=train_args['epochs'], desc='Training Progress') as pbar:\n",
    "    def on_train_epoch_end(trainer):\n",
    "        pbar.update(1)\n",
    "        # Log GPU memory usage\n",
    "        if torch.cuda.is_available():\n",
    "            mem_used = torch.cuda.memory_reserved(0) / 1024**2\n",
    "            mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            print(f\"\\nGPU Memory Used: {mem_used:.0f}MB / {mem_total:.1f}GB\")\n",
    "    \n",
    "    results = model.train(\n",
    "        **train_args\n",
    "    )\n",
    "\n",
    "# Print detailed results summary\n",
    "print(\"\\n=== Training Results ===\")\n",
    "metrics = results.results_dict\n",
    "print(\"\\nAccuracy Metrics:\")\n",
    "print(f\"mAP50-95: {metrics.get('metrics/mAP50-95(B)', 'N/A')} (primary metric)\")\n",
    "print(f\"mAP50: {metrics.get('metrics/mAP50(B)', 'N/A')}\")\n",
    "print(f\"Precision: {metrics.get('metrics/precision(B)', 'N/A')}\")\n",
    "print(f\"Recall: {metrics.get('metrics/recall(B)', 'N/A')}\")\n",
    "\n",
    "print(\"\\nTraining Statistics:\")\n",
    "print(f\"Best epoch: {results.best_epoch}\")\n",
    "print(f\"Final epoch: {results.epoch}\")\n",
    "if results.epoch < train_args['epochs']:\n",
    "    print(\"Note: Training stopped early due to patience criterion\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = os.path.join(train_args['project'], train_args['name'], 'weights/best.pt')\n",
    "print(f\"\\nBest model saved to: {final_model_path}\")\n",
    "\n",
    "# Plot results\n",
    "print(\"\\nGenerating training plots...\")\n",
    "fig = results.plot_results()\n",
    "print(\"Training plots have been saved to the project directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cf86df",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-06T05:37:37.967141Z",
     "iopub.status.idle": "2025-10-06T05:37:37.967958Z",
     "shell.execute_reply": "2025-10-06T05:37:37.967800Z",
     "shell.execute_reply.started": "2025-10-06T05:37:37.967786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 6. Model Evaluation ---\n",
    "print(\"\\n=== Starting Model Evaluation ===\")\n",
    "\n",
    "# Validate on validation set\n",
    "print(\"\\nRunning validation...\")\n",
    "val_results = model.val(\n",
    "    data=yaml_path,\n",
    "    split='val',\n",
    "    conf=0.25,  # Confidence threshold\n",
    "    iou=0.45,   # NMS IOU threshold\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Print detailed metrics\n",
    "metrics = val_results.results_dict\n",
    "print(\"\\nDetailed Evaluation Metrics:\")\n",
    "print(\"1. Average Precision (AP):\")\n",
    "print(f\"  - mAP50-95: {metrics['metrics/mAP50-95']:.4f}\")\n",
    "print(f\"  - mAP50: {metrics['metrics/mAP50']:.4f}\")\n",
    "print(f\"  - mAP75: {metrics['metrics/mAP75']:.4f}\")\n",
    "\n",
    "print(\"\\n2. Precision/Recall:\")\n",
    "print(f\"  - Precision: {metrics['metrics/precision']:.4f}\")\n",
    "print(f\"  - Recall: {metrics['metrics/recall']:.4f}\")\n",
    "\n",
    "print(\"\\n3. Performance:\")\n",
    "print(f\"  - Speed: {metrics.get('speed/inference', 'N/A')} ms per image\")\n",
    "print(f\"  - Total images: {val_results.need}\")\n",
    "print(f\"  - Processed images: {val_results.done}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "val_results.plot_confusion_matrix()\n",
    "print(\"\\nConfusion matrix has been saved to the project directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c8f5e1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-06T05:37:37.969196Z",
     "iopub.status.idle": "2025-10-06T05:37:37.969688Z",
     "shell.execute_reply": "2025-10-06T05:37:37.969549Z",
     "shell.execute_reply.started": "2025-10-06T05:37:37.969531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 7. Visualize Predictions ---\n",
    "print(\"\\n=== Generating Visualizations ===\")\n",
    "\n",
    "# Create directories\n",
    "vis_output_dir = \"./balloon_test_visualizations\"\n",
    "os.makedirs(vis_output_dir, exist_ok=True)\n",
    "\n",
    "# Get validation images\n",
    "val_images_dir = os.path.join(DATASET_DIR, 'images/val')\n",
    "val_images = list(Path(val_images_dir).glob('*.jpg'))\n",
    "print(f\"\\nFound {len(val_images)} validation images\")\n",
    "\n",
    "# Select random samples\n",
    "num_samples = min(20, len(val_images))\n",
    "sample_images = random.sample(val_images, num_samples)\n",
    "\n",
    "print(f\"\\nProcessing {num_samples} random samples...\")\n",
    "for i, img_path in enumerate(sample_images, 1):\n",
    "    print(f\"\\nImage {i}/{num_samples}: {img_path.name}\")\n",
    "    \n",
    "    # Run prediction\n",
    "    results = model.predict(\n",
    "        source=str(img_path),\n",
    "        save=True,          # Save annotated images\n",
    "        save_txt=True,      # Save predictions as txt\n",
    "        conf=0.25,          # Confidence threshold\n",
    "        iou=0.45,          # NMS IOU threshold\n",
    "        line_width=2,       # Box thickness\n",
    "        boxes=True,         # Show boxes\n",
    "        labels=True,        # Show labels\n",
    "        hide_conf=False     # Show confidences\n",
    "    )\n",
    "    \n",
    "    # Print detection summary\n",
    "    r = results[0]  # Get first (only) result\n",
    "    print(f\"Detections: {len(r)} balloons\")\n",
    "    if len(r) > 0:\n",
    "        print(\"Confidence scores:\", [f\"{conf:.2f}\" for conf in r.boxes.conf])\n",
    "    \n",
    "    # Copy visualization\n",
    "    pred_img = Path(r.save_dir) / img_path.name\n",
    "    if pred_img.exists():\n",
    "        dest_path = os.path.join(vis_output_dir, img_path.name)\n",
    "        shutil.copy2(pred_img, dest_path)\n",
    "        print(f\"Saved visualization to: {dest_path}\")\n",
    "\n",
    "print(f\"\\nAll visualizations saved to '{vis_output_dir}'\")\n",
    "print(\"Note: Green boxes show predicted balloons with confidence scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c33d9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-06T05:37:37.970536Z",
     "iopub.status.idle": "2025-10-06T05:37:37.970750Z",
     "shell.execute_reply": "2025-10-06T05:37:37.970660Z",
     "shell.execute_reply.started": "2025-10-06T05:37:37.970651Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 8. Export and Save Model ---\n",
    "print(\"\\n=== Exporting Model ===\")\n",
    "\n",
    "# 1. Save PyTorch model\n",
    "best_model_path = 'balloon_detector_best.pt'\n",
    "shutil.copy2(str(model.best), best_model_path)\n",
    "print(f\"\\n1. PyTorch model saved as '{best_model_path}'\")\n",
    "\n",
    "# 2. Export to ONNX\n",
    "print(\"\\n2. Exporting to ONNX format...\")\n",
    "model.export(format='onnx', dynamic=True)\n",
    "print(\"ONNX model exported successfully\")\n",
    "\n",
    "# 3. Export to TensorRT (if supported)\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        print(\"\\n3. Exporting to TensorRT format...\")\n",
    "        model.export(format='engine', dynamic=True)\n",
    "        print(\"TensorRT model exported successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"TensorRT export failed: {e}\")\n",
    "\n",
    "# 4. Export to CoreML (if on MacOS)\n",
    "if os.uname().sysname == 'Darwin':\n",
    "    try:\n",
    "        print(\"\\n4. Exporting to CoreML format...\")\n",
    "        model.export(format='coreml', dynamic=True)\n",
    "        print(\"CoreML model exported successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"CoreML export failed: {e}\")\n",
    "\n",
    "# Save training plots\n",
    "print(\"\\nGenerating training plots...\")\n",
    "results.plot_results()\n",
    "print(\"Training plots saved in the project directory\")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n=== Export Summary ===\")\n",
    "print(\"The following files have been created:\")\n",
    "print(f\"1. PyTorch model: {best_model_path}\")\n",
    "print(f\"2. ONNX model: {best_model_path.replace('.pt', '.onnx')}\")\n",
    "print(\"3. Training plots: In the project directory\")\n",
    "print(\"\\nModel is ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8407158,
     "sourceId": 13266819,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8408218,
     "sourceId": 13268335,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
